{
 "metadata": {
  "name": "",
  "signature": "sha256:08974a8da98963b08f703436075b57bb9140e7b52356ecf57bc9037a5fdb41a2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#In the name of God"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First import all the libraries and frameworks that will be used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sc\n",
      "from prettyprint import pp\n",
      "import nltk\n",
      "import os\n",
      "import textract\n",
      "from __future__ import division\n",
      "import re\n",
      "from nltk import word_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Next we extract the name of the classes and also the list of all of the documents in each class.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root_path = 'E:/University Central/Modern Information Retrieval/Project/Project Phase 2/20_newsgroup/'\n",
      "#top_view folders\n",
      "folders = [root_path + folder + '/' for folder in os.listdir(root_path)]\n",
      "\n",
      "#there are only 4 classes\n",
      "class_titles = os.listdir(root_path)\n",
      "\n",
      "#list of all the files belonging to each class\n",
      "files = {}\n",
      "for folder, title in zip(folders, class_titles):\n",
      "    files[title] = [folder + f for f in os.listdir(folder)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the tokenizing part, all non-alphabetic characters are removed. Remaining word tokens are the first set of features that will be used.\n",
      "\n",
      "To tokenize a document file to alphabetic words, `tokenizeDoc` will be used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# to tokenize a document file to alphabetic tokens use this function.\n",
      "# doc_address: path to the file that is going to be tokenized\n",
      "# min_len: minimum length of a token. Default value is zero, it should always be non-negative.\n",
      "# remove_numerics: whether to remove the numeric tokens or not\n",
      "from string import punctuation, digits\n",
      "\n",
      "pattern = re.compile(r'([a-zA-Z]+|[0-9]+(\\.[0-9]+)?)')\n",
      "\n",
      "def tokenizeDoc(doc_address, min_len = 0, remove_numerics=True):\n",
      "    tokens = []\n",
      "    try:\n",
      "        f = open(doc_address)\n",
      "        raw = f.read().lower()\n",
      "        text = pattern.sub(r' \\1 ', raw.replace('\\n', ' '))\n",
      "        text_translated = ''\n",
      "        if remove_numerics:\n",
      "            text_translated = text.translate(None, punctuation + digits)\n",
      "        else:\n",
      "            text_translated = text.translate(None, punctuation)\n",
      "        tokens = [word for word in text_translated.split(' ') if (word and len(word) > min_len)]\n",
      "        f.close()\n",
      "    except:\n",
      "        print \"Error: %s couldn't be opened!\", doc_address\n",
      "    finally:\n",
      "        return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Tokenization playground"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# docTokens = tokenizeDoc(files[class_titles[0]][20])\n",
      "# pp (docTokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each line starts with few meta-data lines.\n",
      "\n",
      "This methods tries to extract them, since they can be used as additional features for the classification task.\n",
      "\n",
      "1. Xref: ' '-separated list of subjects\n",
      "2. Path: '!'-separated list\n",
      "3. Newsgroups: ','-separated list\n",
      "4. Subject: a phrase describing the subject of the document\n",
      "5. Lines: a number in this line shows the total number of lines of the document text body\n",
      "\n",
      "**for now I don't see any significant boost on scores due to this features. So I ignore them for now**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tag_lines = ['Path: ', 'Xref: ', 'Newsgroups: ', 'Subject: ']\n",
      "# tag_delimiters = ['!', ' ', ',', ' ']\n",
      "# f = open(files[class_titles[0]][-1])\n",
      "# raw = f.readlines()\n",
      "# for l, d in zip(tag_lines, tag_delimiters):\n",
      "#     ll = [line for line in raw if line.startswith(l)]\n",
      "#     if(len(ll) > 0):\n",
      "#         pp(len(ll))\n",
      "#         pp(ll[0][len(l):].split(d))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Create dictionary from a pool of tokens"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def createDictionary(classes, tokens_pool):\n",
      "    \"\"\"\n",
      "    this method will create a dictionary out of the tokens_pool it has been provided.\n",
      "    classes: this is a list of the names of the classes documents belong to\n",
      "    tokens_pool: a pool (in fact implemented as a dictionary) of tokens. Each value of the dictionary is an list of lists,\n",
      "                 each list belonging to a document in the corresponding class that has a list of tokens\n",
      "   \n",
      "   output: as output, this method will return a dictionary with (<term>, termID) key/value pairs. \n",
      "            *Note that the tokens in the dictionary are not sorted, since in the vector space model \n",
      "             that we are going to use, all words are treated equal. We practically believe in justice. Words in \n",
      "             dictionary are tired of all this injustice they have been forced to take for such a long time. Now is \n",
      "             the time to rise and earn the justice that belongs to them. \n",
      "    \"\"\"\n",
      "    classes_set = set([])\n",
      "    for cl in classes:\n",
      "        token_pool = tokens_pool[cl]\n",
      "        class_set = set([])\n",
      "        for tokens_list in token_pool:\n",
      "            class_set |= set(tokens_list)\n",
      "#             for token in tokens_list:\n",
      "#                 class_set.add(token)\n",
      "        classes_set |= class_set\n",
      "    \n",
      "    token_dict = dict(zip(classes_set, range(len(classes_set))))\n",
      "    return token_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####dictionary creation playground"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Create a token pool out of a list of document addresses"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def createTokenPool(classes, paths):\n",
      "    \"\"\"\n",
      "    this method will create a pool of tokens out of the list of paths to documents it will be provided\n",
      "    classes: a list of the names of the classes documents belong to\n",
      "    paths: a dictionary of lists of paths to documents\n",
      "    \n",
      "    output: a dictionary of lists of lists of tokens. each value bin of dictionary is a has a list of lists,\n",
      "            for which each list is of a document and it contains a list of tokens in that document\n",
      "    \"\"\"\n",
      "    token_pool = {}\n",
      "    for cl in classes:\n",
      "        token_pool[cl] = []\n",
      "        for path in paths[cl]:\n",
      "            token_pool[cl].append(tokenizeDoc(path))\n",
      "            \n",
      "    return token_pool"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####token pool generation playground"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    }
   ],
   "metadata": {}
  }
 ]
}