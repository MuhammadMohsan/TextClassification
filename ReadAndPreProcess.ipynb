{
 "metadata": {
  "name": "",
  "signature": "sha256:3558a1eb011ced0cf954a213b1db1d990d332a5bf19a2d546509507d38b42beb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#In the name of God"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First import all the libraries and frameworks that will be used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sc\n",
      "from prettyprint import pp\n",
      "import nltk\n",
      "import os\n",
      "import textract\n",
      "from __future__ import division\n",
      "import re\n",
      "from nltk import word_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Next we extract the name of the classes and also the list of all of the documents in each class.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root_path = 'E:/University Central/Modern Information Retrieval/Project/Project Phase 2/20_newsgroup/'\n",
      "#top_view folders\n",
      "folders = [root_path + folder + '/' for folder in os.listdir(root_path)]\n",
      "\n",
      "#there are only 4 classes\n",
      "class_titles = os.listdir(root_path)\n",
      "\n",
      "#list of all the files belonging to each class\n",
      "files = {}\n",
      "for folder, title in zip(folders, class_titles):\n",
      "    files[title] = [folder + f for f in os.listdir(folder)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the tokenizing part, all non-alphabetic characters are removed. Remaining word tokens are the first set of features that will be used.\n",
      "\n",
      "To tokenize a document file to alphabetic words, `tokenizeDoc` will be used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# to tokenize a document file to alphabetic tokens use this function.\n",
      "# doc_address: path to the file that is going to be tokenized\n",
      "# min_len: minimum length of a token. Default value is zero, it should always be non-negative.\n",
      "def tokenizeDoc(doc_address, min_len = 0):\n",
      "    tokens = []\n",
      "    try:\n",
      "        f = open(doc_address)\n",
      "        raw = f.read().lower()\n",
      "        tokens = [a for a in re.split('\\W|\\d', raw) if len(a) > min_len]\n",
      "        f.close()\n",
      "    except:\n",
      "        print \"Error: %s couldn't be opened!\", doc_address\n",
      "    finally:\n",
      "        return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    }
   ],
   "metadata": {}
  }
 ]
}