{
 "metadata": {
  "name": "",
  "signature": "sha256:87246210961f156496b12a837eb6546be5be7976362b07c03732e4d668fa1780"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#In the name of God"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First import all the libraries and frameworks that will be used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sc\n",
      "from prettyprint import pp\n",
      "import nltk\n",
      "import os\n",
      "import textract\n",
      "from __future__ import division\n",
      "import re\n",
      "from nltk import word_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Next we extract the name of the classes and also the list of all of the documents in each class.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root_path = 'E:/University Central/Modern Information Retrieval/Project/Project Phase 2/20_newsgroup/'\n",
      "#top_view folders\n",
      "folders = [root_path + folder + '/' for folder in os.listdir(root_path)]\n",
      "\n",
      "#there are only 4 classes\n",
      "class_titles = os.listdir(root_path)\n",
      "\n",
      "#list of all the files belonging to each class\n",
      "files = {}\n",
      "for folder, title in zip(folders, class_titles):\n",
      "    files[title] = [folder + f for f in os.listdir(folder)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the tokenizing part, all non-alphabetic characters are removed. Remaining word tokens are the first set of features that will be used.\n",
      "\n",
      "To tokenize a document file to alphabetic words, `tokenizeDoc` will be used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# to tokenize a document file to alphabetic tokens use this function.\n",
      "# doc_address: path to the file that is going to be tokenized\n",
      "# min_len: minimum length of a token. Default value is zero, it should always be non-negative.\n",
      "# remove_numerics: whether to remove the numeric tokens or not\n",
      "from string import punctuation, digits\n",
      "\n",
      "pattern = re.compile(r'([a-zA-Z]+|[0-9]+(\\.[0-9]+)?)')\n",
      "\n",
      "def tokenizeDoc(doc_address, min_len = 0, remove_numerics=True):\n",
      "    tokens = []\n",
      "    try:\n",
      "        f = open(doc_address)\n",
      "        raw = f.read().lower()\n",
      "        text = pattern.sub(r' \\1 ', raw.replace('\\n', ' '))\n",
      "        text_translated = ''\n",
      "        if remove_numerics:\n",
      "            text_translated = text.translate(None, punctuation + digits)\n",
      "        else:\n",
      "            text_translated = text.translate(None, punctuation)\n",
      "        tokens = [word for word in text_translated.split(' ') if (word and len(word) > min_len)]\n",
      "        f.close()\n",
      "    except:\n",
      "        print \"Error: %s couldn't be opened!\", doc_address\n",
      "    finally:\n",
      "        return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Tokenization playground"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# docTokens = tokenizeDoc(files[class_titles[0]][20])\n",
      "# pp (docTokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each line starts with few meta-data lines.\n",
      "\n",
      "This methods tries to extract them, since they can be used as additional features for the classification task.\n",
      "\n",
      "1. Xref: ' '-separated list of subjects\n",
      "2. Path: '!'-separated list\n",
      "3. Newsgroups: ','-separated list\n",
      "4. Subject: a phrase describing the subject of the document\n",
      "5. Lines: a number in this line shows the total number of lines of the document text body\n",
      "\n",
      "**for now I don't see any significant boost on scores due to this features. So I ignore them for now**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tag_lines = ['Path: ', 'Xref: ', 'Newsgroups: ', 'Subject: ']\n",
      "# tag_delimiters = ['!', ' ', ',', ' ']\n",
      "# f = open(files[class_titles[0]][-1])\n",
      "# raw = f.readlines()\n",
      "# for l, d in zip(tag_lines, tag_delimiters):\n",
      "#     ll = [line for line in raw if line.startswith(l)]\n",
      "#     if(len(ll) > 0):\n",
      "#         pp(len(ll))\n",
      "#         pp(ll[0][len(l):].split(d))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}